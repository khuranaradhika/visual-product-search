{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64cf37c",
   "metadata": {},
   "source": [
    "Python note book to test CLIP with CIFAR-10 data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706e077",
   "metadata": {},
   "source": [
    "## Project Plan: CLIP with CIFAR-10 and FAISS\n",
    "\n",
    "### 1. Setup and Initialization\n",
    "- Install dependencies (`transformers`, `datasets`, `faiss-cpu` or `faiss-gpu`, `torch`, `torchvision`, `scikit-learn`, `matplotlib`).\n",
    "- Import necessary libraries.\n",
    "- Check and configure device (CPU/GPU).\n",
    "- **Update:** Ensure reproducible results by setting random seeds.\n",
    "\n",
    "### 2. Data Loading (CIFAR-10)\n",
    "- Load the CIFAR-10 dataset from Hugging Face `datasets`.\n",
    "- Split into test/train sets (focusing on test set for evaluation).\n",
    "- Visualize a few random samples to verify data.\n",
    "- **Optimization:** Use a `DataLoader` for efficient batch processing during embedding generation.\n",
    "\n",
    "### 3. Model Loading (CLIP)\n",
    "- Load pretrained CLIP model and processor from `transformers` (e.g., `openai/clip-vit-base-patch32`).\n",
    "- **Prompt Engineering:** Instead of raw labels (\"dog\"), use prompt templates (e.g., \"a photo of a dog\", \"a low resolution photo of a dog\") to improve zero-shot accuracy on CIFAR-10's low-res images.\n",
    "\n",
    "### 4. Embedding Generation (Batched)\n",
    "- **Image Embeddings:** Pass CIFAR-10 images through the CLIP vision model in batches to get feature vectors.\n",
    "- **Text Embeddings:** Encode the class labels using the CLIP text model.\n",
    "- **Normalization:** Normalize all vectors to unit length (L2 norm) so that Inner Product (IP) search in FAISS equals Cosine Similarity.\n",
    "\n",
    "### 5. Indexing with FAISS\n",
    "- Initialize a FAISS index (e.g., `IndexFlatIP` for inner product/cosine similarity).\n",
    "- **Optimization:** If dataset grows, consider `IndexIVFFlat` for faster approximate search.\n",
    "- Add image embeddings to the FAISS index.\n",
    "\n",
    "### 6. Retrieval Pipeline\n",
    "- **Zero-Shot Classification:** Query the image-index with text vectors to find the best matching image class.\n",
    "- **Visual Search (Image-to-Image):** Use a query image to find the nearest neighbors in the dataset (Simulating \"product search\").\n",
    "\n",
    "### 7. Evaluation & Metrics\n",
    "- **Quantitative:**\n",
    "    - Zero-Shot Accuracy: Percentage of correctly predicted class labels.\n",
    "    - Top-k Accuracy: (e.g., Top-1, Top-5).\n",
    "- **Qualitative:**\n",
    "    - Visualize Top-K retrieval results for specific queries (Show query image next to retrieved results).\n",
    "    - Generate a **Confusion Matrix** to visualize misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af281c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Initialization\n",
    "# Install required libraries if not already installed\n",
    "# %pip install transformers datasets faiss-cpu torch torchvision scikit-learn matplotlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# specific for mac m1/m2 chips\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loading (CIFAR-10)\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "# We'll use the test split for evaluation to save time, \n",
    "# but you can use 'train' for building a larger index if needed.\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Define the class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Visualize a few samples\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(test_dataset))\n",
    "    image = test_dataset[idx]['img']\n",
    "    label = test_dataset[idx]['label']\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_title(class_names[label])\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Loading (CLIP)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Load model and processor\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "print(f\"Model {model_id} loaded successfully on {device}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
